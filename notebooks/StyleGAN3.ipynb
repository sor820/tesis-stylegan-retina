{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itV2xoKaqmeC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30d21d11-35a5-4276-f9c9-0832a6b2b4f6",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Cloning into 'stylegan3'...\n",
            "remote: Enumerating objects: 212, done.\u001b[K\n",
            "remote: Counting objects: 100% (163/163), done.\u001b[K\n",
            "remote: Compressing objects: 100% (73/73), done.\u001b[K\n",
            "remote: Total 212 (delta 99), reused 90 (delta 90), pack-reused 49 (from 1)\u001b[K\n",
            "Receiving objects: 100% (212/212), 4.16 MiB | 18.45 MiB/s, done.\n",
            "Resolving deltas: 100% (108/108), done.\n",
            "Collecting ninja\n",
            "  Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
            "Collecting opensimplex\n",
            "  Downloading opensimplex-0.4.5.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.12/dist-packages (from opensimplex) (2.0.2)\n",
            "Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opensimplex-0.4.5.1-py3-none-any.whl (267 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.0/268.0 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: opensimplex, ninja\n",
            "Successfully installed ninja-1.13.0 opensimplex-0.4.5.1\n"
          ]
        }
      ],
      "source": [
        "# ==========================================================\n",
        "# 1. PREPARACIÓN Y MONTAJE\n",
        "# ==========================================================\n",
        "import os\n",
        "import shutil\n",
        "import torch\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "if not os.path.exists('/content/stylegan3'):\n",
        "    !git clone https://github.com/NVlabs/stylegan3.git\n",
        "\n",
        "!pip install ninja opensimplex\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# 2. PARCHE DE LIBRERÍA (CORRECCIÓN DEFINITIVA DE BETAS)\n",
        "# Modificamos la librería de PyTorch en el sistema\n",
        "# ==========================================================\n",
        "print(\"Aplicando parche preventivo en la librería PyTorch...\")\n",
        "\n",
        "adam_file = \"/usr/local/lib/python3.12/dist-packages/torch/optim/adam.py\"\n",
        "if os.path.exists(adam_file):\n",
        "    with open(adam_file, 'r') as f:\n",
        "        content = f.read()\n",
        "\n",
        "    # Buscamos la validación que lanza el error y la neutralizamos\n",
        "    old_check = 'if not 0.0 <= betas[0] < 1.0:'\n",
        "    new_check = 'betas = (float(betas[0]), float(betas[1]))\\n        if not 0.0 <= betas[0] < 1.0:'\n",
        "\n",
        "    if 'float(betas[0])' not in content:\n",
        "        content = content.replace(old_check, new_check)\n",
        "        with open(adam_file, 'w') as f:\n",
        "            f.write(content)\n",
        "        print(\"✅ Librería PyTorch parcheada con éxito.\")"
      ],
      "metadata": {
        "id": "Yb2v2WgMquUo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b95b4286-8688-4b0d-e02e-2532138b53f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aplicando parche preventivo en la librería PyTorch...\n",
            "✅ Librería PyTorch parcheada con éxito.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# 3. PARCHE DE OPS (MODO LENTO / COMPATIBILIDAD)\n",
        "# ==========================================================\n",
        "archivos_ops = [\n",
        "    '/content/stylegan3/torch_utils/ops/bias_act.py',\n",
        "    '/content/stylegan3/torch_utils/ops/upfirdn2d.py',\n",
        "    '/content/stylegan3/torch_utils/ops/filtered_lrelu.py'\n",
        "]\n",
        "for ruta in archivos_ops:\n",
        "    if os.path.exists(ruta):\n",
        "        with open(ruta, 'r') as f:\n",
        "            codigo = f.read()\n",
        "        codigo = codigo.replace(\"if impl == 'cuda' and x.device.type == 'cuda' and _init():\", \"if False:\")\n",
        "        with open(ruta, 'w') as f:\n",
        "            f.write(codigo)"
      ],
      "metadata": {
        "id": "h0khOyCfq-C1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==========================================================\n",
        "# 4. RUTAS Y LANZAMIENTO\n",
        "# ==========================================================\n",
        "drive_zip = \"/content/drive/MyDrive/Recursos/Conjuntos_de_datos/dataset_526.zip\"\n",
        "#drive_pkl = \"/content/drive/MyDrive/ModelosIA/StyleGAN3/stylegan3-t-ffhqu-256x256.pkl\"\n",
        "#drive_pkl = \"/content/drive/MyDrive/training-runs-sg3/00048-stylegan3-t-dataset_final-gpus1-batch16-gamma2.8/network-snapshot-000100.pkl\"\n",
        "#drive_pkl = \"/content/drive/MyDrive/ModelosIA/StyleGAN3/stylegan3-t-ffhqu-256x256.pkl\"\n",
        "local_zip = \"/content/dataset_final.zip\"\n",
        "local_pkl = \"/content/stylegan3_local.pkl\"\n",
        "OUT_DIR = \"/content/drive/MyDrive/training-runs-sg3\"\n",
        "\n",
        "shutil.copyfile(drive_zip, local_zip)\n",
        "shutil.copyfile(drive_pkl, local_pkl)\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "!python /content/stylegan3/train.py \\\n",
        "    --outdir={OUT_DIR} \\\n",
        "    --data={local_zip} \\\n",
        "    --resume={local_pkl} \\\n",
        "    --cfg=stylegan3-t \\\n",
        "    --gpus=1 \\\n",
        "    --batch=16 \\\n",
        "    --batch-gpu=4 \\\n",
        "    --gamma=2.8 \\\n",
        "    --cbase=16384 \\\n",
        "    --cmax=512 \\\n",
        "    --mirror=1 \\\n",
        "    --aug=ada \\\n",
        "    --snap=1 \\\n",
        "    --metrics=none \\\n",
        "    --kimg=5000"
      ],
      "metadata": {
        "id": "R7w-sQWnrAED",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19f6af56-e040-46ef-ad20-85e4c8c1c928",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training options:\n",
            "{\n",
            "  \"G_kwargs\": {\n",
            "    \"class_name\": \"training.networks_stylegan3.Generator\",\n",
            "    \"z_dim\": 512,\n",
            "    \"w_dim\": 512,\n",
            "    \"mapping_kwargs\": {\n",
            "      \"num_layers\": 2\n",
            "    },\n",
            "    \"channel_base\": 16384,\n",
            "    \"channel_max\": 512,\n",
            "    \"magnitude_ema_beta\": 0.9994456359721023\n",
            "  },\n",
            "  \"D_kwargs\": {\n",
            "    \"class_name\": \"training.networks_stylegan2.Discriminator\",\n",
            "    \"block_kwargs\": {\n",
            "      \"freeze_layers\": 0\n",
            "    },\n",
            "    \"mapping_kwargs\": {},\n",
            "    \"epilogue_kwargs\": {\n",
            "      \"mbstd_group_size\": 4\n",
            "    },\n",
            "    \"channel_base\": 16384,\n",
            "    \"channel_max\": 512\n",
            "  },\n",
            "  \"G_opt_kwargs\": {\n",
            "    \"class_name\": \"torch.optim.Adam\",\n",
            "    \"betas\": [\n",
            "      0,\n",
            "      0.99\n",
            "    ],\n",
            "    \"eps\": 1e-08,\n",
            "    \"lr\": 0.0025\n",
            "  },\n",
            "  \"D_opt_kwargs\": {\n",
            "    \"class_name\": \"torch.optim.Adam\",\n",
            "    \"betas\": [\n",
            "      0,\n",
            "      0.99\n",
            "    ],\n",
            "    \"eps\": 1e-08,\n",
            "    \"lr\": 0.002\n",
            "  },\n",
            "  \"loss_kwargs\": {\n",
            "    \"class_name\": \"training.loss.StyleGAN2Loss\",\n",
            "    \"r1_gamma\": 2.8,\n",
            "    \"blur_init_sigma\": 0\n",
            "  },\n",
            "  \"data_loader_kwargs\": {\n",
            "    \"pin_memory\": true,\n",
            "    \"prefetch_factor\": 2,\n",
            "    \"num_workers\": 3\n",
            "  },\n",
            "  \"training_set_kwargs\": {\n",
            "    \"class_name\": \"training.dataset.ImageFolderDataset\",\n",
            "    \"path\": \"/content/dataset_final.zip\",\n",
            "    \"use_labels\": false,\n",
            "    \"max_size\": 526,\n",
            "    \"xflip\": true,\n",
            "    \"resolution\": 256,\n",
            "    \"random_seed\": 0\n",
            "  },\n",
            "  \"num_gpus\": 1,\n",
            "  \"batch_size\": 16,\n",
            "  \"batch_gpu\": 4,\n",
            "  \"metrics\": [],\n",
            "  \"total_kimg\": 5000,\n",
            "  \"kimg_per_tick\": 4,\n",
            "  \"image_snapshot_ticks\": 1,\n",
            "  \"network_snapshot_ticks\": 1,\n",
            "  \"random_seed\": 0,\n",
            "  \"ema_kimg\": 5.0,\n",
            "  \"augment_kwargs\": {\n",
            "    \"class_name\": \"training.augment.AugmentPipe\",\n",
            "    \"xflip\": 1,\n",
            "    \"rotate90\": 1,\n",
            "    \"xint\": 1,\n",
            "    \"scale\": 1,\n",
            "    \"rotate\": 1,\n",
            "    \"aniso\": 1,\n",
            "    \"xfrac\": 1,\n",
            "    \"brightness\": 1,\n",
            "    \"contrast\": 1,\n",
            "    \"lumaflip\": 1,\n",
            "    \"hue\": 1,\n",
            "    \"saturation\": 1\n",
            "  },\n",
            "  \"ada_target\": 0.6,\n",
            "  \"resume_pkl\": \"/content/stylegan3_local.pkl\",\n",
            "  \"ada_kimg\": 100,\n",
            "  \"ema_rampup\": null,\n",
            "  \"run_dir\": \"/content/drive/MyDrive/training-runs-sg3/00049-stylegan3-t-dataset_final-gpus1-batch16-gamma2.8\"\n",
            "}\n",
            "\n",
            "Output directory:    /content/drive/MyDrive/training-runs-sg3/00049-stylegan3-t-dataset_final-gpus1-batch16-gamma2.8\n",
            "Number of GPUs:      1\n",
            "Batch size:          16 images\n",
            "Training duration:   5000 kimg\n",
            "Dataset path:        /content/dataset_final.zip\n",
            "Dataset size:        526 images\n",
            "Dataset resolution:  256\n",
            "Dataset labels:      False\n",
            "Dataset x-flips:     True\n",
            "\n",
            "Creating output directory...\n",
            "Launching processes...\n",
            "/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
            "  self.setter(val)\n",
            "Loading training set...\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/sampler.py:74: UserWarning: `data_source` argument is not used and will be removed in 2.2.0.You may still have custom implementation that utilizes it.\n",
            "  warnings.warn(\n",
            "\n",
            "Num images:  1052\n",
            "Image shape: [3, 256, 256]\n",
            "Label shape: [0]\n",
            "\n",
            "Constructing networks...\n",
            "Resuming from \"/content/stylegan3_local.pkl\"\n",
            "\n",
            "Generator                     Parameters  Buffers  Output shape        Datatype\n",
            "---                           ---         ---      ---                 ---     \n",
            "mapping.fc0                   262656      -        [4, 512]            float32 \n",
            "mapping.fc1                   262656      -        [4, 512]            float32 \n",
            "mapping                       -           512      [4, 16, 512]        float32 \n",
            "synthesis.input.affine        2052        -        [4, 4]              float32 \n",
            "synthesis.input               262144      1545     [4, 512, 36, 36]    float32 \n",
            "synthesis.L0_36_512.affine    262656      -        [4, 512]            float32 \n",
            "synthesis.L0_36_512           2359808     25       [4, 512, 36, 36]    float32 \n",
            "synthesis.L1_36_512.affine    262656      -        [4, 512]            float32 \n",
            "synthesis.L1_36_512           2359808     25       [4, 512, 36, 36]    float32 \n",
            "synthesis.L2_36_512.affine    262656      -        [4, 512]            float32 \n",
            "synthesis.L2_36_512           2359808     25       [4, 512, 36, 36]    float32 \n",
            "synthesis.L3_52_512.affine    262656      -        [4, 512]            float32 \n",
            "synthesis.L3_52_512           2359808     37       [4, 512, 52, 52]    float16 \n",
            "synthesis.L4_52_512.affine    262656      -        [4, 512]            float32 \n",
            "synthesis.L4_52_512           2359808     25       [4, 512, 52, 52]    float16 \n",
            "synthesis.L5_84_512.affine    262656      -        [4, 512]            float32 \n",
            "synthesis.L5_84_512           2359808     37       [4, 512, 84, 84]    float16 \n",
            "synthesis.L6_84_512.affine    262656      -        [4, 512]            float32 \n",
            "synthesis.L6_84_512           2359808     25       [4, 512, 84, 84]    float16 \n",
            "synthesis.L7_148_362.affine   262656      -        [4, 512]            float32 \n",
            "synthesis.L7_148_362          1668458     37       [4, 362, 148, 148]  float16 \n",
            "synthesis.L8_148_256.affine   185706      -        [4, 362]            float32 \n",
            "synthesis.L8_148_256          834304      25       [4, 256, 148, 148]  float16 \n",
            "synthesis.L9_148_181.affine   131328      -        [4, 256]            float32 \n",
            "synthesis.L9_148_181          417205      25       [4, 181, 148, 148]  float16 \n",
            "synthesis.L10_276_128.affine  92853       -        [4, 181]            float32 \n",
            "synthesis.L10_276_128         208640      37       [4, 128, 276, 276]  float16 \n",
            "synthesis.L11_276_91.affine   65664       -        [4, 128]            float32 \n",
            "synthesis.L11_276_91          104923      25       [4, 91, 276, 276]   float16 \n",
            "synthesis.L12_276_64.affine   46683       -        [4, 91]             float32 \n",
            "synthesis.L12_276_64          52480       25       [4, 64, 276, 276]   float16 \n",
            "synthesis.L13_256_64.affine   32832       -        [4, 64]             float32 \n",
            "synthesis.L13_256_64          36928       25       [4, 64, 256, 256]   float16 \n",
            "synthesis.L14_256_3.affine    32832       -        [4, 64]             float32 \n",
            "synthesis.L14_256_3           195         1        [4, 3, 256, 256]    float16 \n",
            "synthesis                     -           -        [4, 3, 256, 256]    float32 \n",
            "---                           ---         ---      ---                 ---     \n",
            "Total                         23320443    2456     -                   -       \n",
            "\n",
            "\n",
            "Discriminator  Parameters  Buffers  Output shape        Datatype\n",
            "---            ---         ---      ---                 ---     \n",
            "b256.fromrgb   256         16       [4, 64, 256, 256]   float16 \n",
            "b256.skip      8192        16       [4, 128, 128, 128]  float16 \n",
            "b256.conv0     36928       16       [4, 64, 256, 256]   float16 \n",
            "b256.conv1     73856       16       [4, 128, 128, 128]  float16 \n",
            "b256           -           16       [4, 128, 128, 128]  float16 \n",
            "b128.skip      32768       16       [4, 256, 64, 64]    float16 \n",
            "b128.conv0     147584      16       [4, 128, 128, 128]  float16 \n",
            "b128.conv1     295168      16       [4, 256, 64, 64]    float16 \n",
            "b128           -           16       [4, 256, 64, 64]    float16 \n",
            "b64.skip       131072      16       [4, 512, 32, 32]    float16 \n",
            "b64.conv0      590080      16       [4, 256, 64, 64]    float16 \n",
            "b64.conv1      1180160     16       [4, 512, 32, 32]    float16 \n",
            "b64            -           16       [4, 512, 32, 32]    float16 \n",
            "b32.skip       262144      16       [4, 512, 16, 16]    float16 \n",
            "b32.conv0      2359808     16       [4, 512, 32, 32]    float16 \n",
            "b32.conv1      2359808     16       [4, 512, 16, 16]    float16 \n",
            "b32            -           16       [4, 512, 16, 16]    float16 \n",
            "b16.skip       262144      16       [4, 512, 8, 8]      float32 \n",
            "b16.conv0      2359808     16       [4, 512, 16, 16]    float32 \n",
            "b16.conv1      2359808     16       [4, 512, 8, 8]      float32 \n",
            "b16            -           16       [4, 512, 8, 8]      float32 \n",
            "b8.skip        262144      16       [4, 512, 4, 4]      float32 \n",
            "b8.conv0       2359808     16       [4, 512, 8, 8]      float32 \n",
            "b8.conv1       2359808     16       [4, 512, 4, 4]      float32 \n",
            "b8             -           16       [4, 512, 4, 4]      float32 \n",
            "b4.mbstd       -           -        [4, 513, 4, 4]      float32 \n",
            "b4.conv        2364416     16       [4, 512, 4, 4]      float32 \n",
            "b4.fc          4194816     -        [4, 512]            float32 \n",
            "b4.out         513         -        [4, 1]              float32 \n",
            "---            ---         ---      ---                 ---     \n",
            "Total          24001089    416      -                   -       \n",
            "\n",
            "Setting up augmentation...\n",
            "Distributing across 1 GPUs...\n",
            "Setting up training phases...\n",
            "Exporting sample images...\n",
            "Initializing logs...\n",
            "2026-01-08 22:10:25.643033: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2026-01-08 22:10:25.659028: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1767910225.676806    1993 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1767910225.682113    1993 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1767910225.695764    1993 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1767910225.695787    1993 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1767910225.695790    1993 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1767910225.695793    1993 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-01-08 22:10:25.699949: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Training for 5000 kimg...\n",
            "\n",
            "tick 0     kimg 0.0      time 2m 07s       sec/tick 58.3    sec/kimg 3642.02 maintenance 68.9   cpumem 3.66   gpumem 34.44  reserved 35.77  augment 0.000\n",
            "tick 1     kimg 4.0      time 16m 07s      sec/tick 804.2   sec/kimg 201.06  maintenance 36.0   cpumem 4.10   gpumem 15.34  reserved 16.53  augment 0.023\n",
            "tick 2     kimg 8.0      time 31m 23s      sec/tick 879.2   sec/kimg 219.81  maintenance 36.5   cpumem 4.11   gpumem 15.35  reserved 16.47  augment 0.049\n",
            "tick 3     kimg 12.0     time 46m 44s      sec/tick 884.3   sec/kimg 221.08  maintenance 36.5   cpumem 4.10   gpumem 15.37  reserved 16.47  augment 0.069\n",
            "tick 4     kimg 16.0     time 1h 02m 18s   sec/tick 897.6   sec/kimg 224.41  maintenance 36.6   cpumem 4.11   gpumem 15.37  reserved 16.47  augment 0.085\n",
            "tick 5     kimg 20.0     time 1h 18m 28s   sec/tick 933.3   sec/kimg 233.34  maintenance 36.5   cpumem 4.11   gpumem 15.38  reserved 16.50  augment 0.107\n",
            "tick 6     kimg 24.0     time 1h 34m 20s   sec/tick 915.8   sec/kimg 228.96  maintenance 36.5   cpumem 4.11   gpumem 15.39  reserved 16.50  augment 0.120\n",
            "tick 7     kimg 28.0     time 1h 51m 02s   sec/tick 965.3   sec/kimg 241.33  maintenance 36.6   cpumem 4.10   gpumem 15.40  reserved 16.50  augment 0.132\n",
            "tick 8     kimg 32.0     time 2h 07m 07s   sec/tick 928.0   sec/kimg 231.99  maintenance 36.5   cpumem 4.10   gpumem 15.38  reserved 16.50  augment 0.135\n",
            "tick 9     kimg 36.0     time 2h 22m 58s   sec/tick 914.1   sec/kimg 228.52  maintenance 36.6   cpumem 4.10   gpumem 15.40  reserved 16.50  augment 0.147\n",
            "tick 10    kimg 40.0     time 2h 39m 43s   sec/tick 968.7   sec/kimg 242.17  maintenance 36.6   cpumem 4.10   gpumem 15.39  reserved 16.50  augment 0.154\n",
            "tick 11    kimg 44.0     time 2h 55m 35s   sec/tick 916.1   sec/kimg 229.02  maintenance 36.5   cpumem 4.10   gpumem 15.40  reserved 16.50  augment 0.160\n",
            "tick 12    kimg 48.0     time 3h 11m 57s   sec/tick 945.0   sec/kimg 236.26  maintenance 36.5   cpumem 4.10   gpumem 15.40  reserved 16.50  augment 0.156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3KztIZ62PiAZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}